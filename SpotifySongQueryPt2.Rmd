---
title: "Spotify Song Query Part 2"
author: "Jessica Cheng and Luke Novak"
date: "April 17, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Set up connection to db

```{r}
library(dplyr)
library(spotifyr)
library(RPostgreSQL)
library(lubridate)
library(stringr)
require(DBI)

drv <- dbDriver("PostgreSQL")
db <- dbConnect(drv) #DB DATA GOES HERE

```

# Spotify album tracks queries

We want to obtain specific information such as key, duration, valence, popularity and tempo from each track given an album. Our question involves studying how certain features such as key and tempo changes over time in. We decided to obtain this information from the tracks in the most popular albums.

## Test queries and data
```{r}
# search tracks in album
search_spotify('Twin Fantasy', 'track')

# get album info by id
get_new_album_info('20U1UWeGcGq7JVW0tf8yfH')

#test function
test_a_search <- album_search_get_id("Twin Fantasy", "Car Seat Headrest")

# grab info from test data
album_tracks <- get_album_tracks(test_a_search)
track <- album_tracks
track_name <- track$name
track_id <- track$id
track_dur <- track$duration_ms
track_audio <- get_track_audio_features(track_id)
track_key <- track_audio$key
track_dance <- track_audio$danceability
track_valence <- track_audio$valence
track_tempo <- track_audio$tempo

# get test track popularity info
get_track_info <- get_track("3z3BsIVe0RHu7m6J1KZxg1")
get_track_popularity <- get_track_info$popularity
get_track_id <- get_track_info$id

tracks_popularity <- sapply(track_id, function(id)get_track(id)$popularity)

```
# Create album songs table

We decided to store all of the information gathered into a tibble and write it to our song table in our database. Information also stored include track IDs to uniquely identify tracks and the album IDs of the album the track came from. This way, we can use the song data and combine information across multiple tables for further analysis.
```{r}
#function to create tibble containing song info from album id
get_song_info <- function(dbconn, album_id) {
  validRequest <- FALSE
  print(paste("searching for album", album_id))
  while(!validRequest) {
    tryCatch({
      songs <- get_album_tracks(album_id)
      validRequest <- TRUE
    }, error = function(e) {
      print("HTTP Error: perhaps you hit the rate limit? sleeping for 5s")
      Sys.sleep(1)
      print("4")
      Sys.sleep(1)
      print("3") 
      Sys.sleep(1)
      print("2") 
      Sys.sleep(1)
      print("1") 
      Sys.sleep(1)
    })
  }
  song_info <- songs
  song_name <- song_info$name
  song_name <-str_replace_all(song_name, "'", "''")
  song_id <- song_info$id
  song_dur <- song_info$duration_ms
  ## we need this to make one query for all tracks
  song_id_string <- paste(song_id, collapse = ',')
  print(song_id_string)
  song_audio <- get_track_audio_features(song_id_string)
  song_key <- song_audio$key
  song_dance <- song_audio$danceability
  song_valence <- song_audio$valence
  song_tempo <- song_audio$tempo
  song_pop <- get_tracks(song_id_string)$popularity
  result <- tibble(id = song_id,
                   album_id = album_id,
                   name = song_name,
                   popularity = song_pop,
                   duration = song_dur,
                   key = song_key,
                   danceability = song_dance,
                   valence = song_valence,
                   tempo = round(song_tempo))
  dbWriteTable(dbconn, "songs", result, append = TRUE, row.names = F)
  print(paste("Added songs from", album_id))
  return(result)
}

# table with album song data added 
albums_with_id <- dbGetQuery(db, 'SELECT * FROM albums;')
albums_already_scraped <- dbGetQuery(db, 'SELECT album_id FROM songs GROUP BY album_id')
remaining_albums_songs <- anti_join(albums_with_id, albums_already_scraped, by=c("id" = "album_id"))
lapply(remaining_albums_songs$id, FUN = get_song_info, dbconn = db)
```



To analyze the genre and format data we gathered through discogs (and the
python script scraper, we first need to grab our data from the database
and clean it up a little (although the data has already been cleaned)

To do this, we join the formats graph with the albums graph so we can see what
albums were released in what format
```{r get data}
formats <- dbGetQuery(db, "SELECT * FROM releases;")
all_albums <- dbGetQuery(db, "SELECT * FROM albums;")
formats_joined <- inner_join(formats, all_albums, by = 'id')
```


## Format analysis
To analyze the format of graphs over time (this was one of the main goals of 
the project) we take the formats of albums and the year in which they were 
released and group them together, summarizing them to get the count of each
format per year. We then proceed to remove outliers, and do a simple imputation
of data for the rest of 2019 to normalize it. Finally, we plot the data.
```{r format analysis}
count_format_per_year <- formats_joined %>% select(format, year) %>% 
  group_by(format, year) %>% summarize(total = n())

## We need to remove some outliers, like CDs prior to their inventin in 1982
count_format_per_year <- count_format_per_year %>% 
  filter(format != 'CD' | year > 1981)
## multiply the current year by 3 to extrapolate out
count_format_per_year <- count_format_per_year %>% 
  mutate(total = ifelse(year == 2019, total * 4, total))

ggplot(count_format_per_year, aes(x = year, y=total, color=format)) + geom_line()

```
As we can see, the format of albums has changed greatly over time. While Vinyl
is the only true contender for the beginning of the graph, we see the rapid
rise in CDs as time goes on, and the eventual downfall of CDs as well as the
vinyl revival in the mid 2000s. Finally we see the rise of digitial in the
2010s as well.

It is worth noting that Discogs is a biased site, and is thus heavily skewed
towards physical releases (vinyl especially), and so we must consider that when 
tagging the primary release of an album, users will try to refrain from using
the file tag unless no physical release is available. Thus, the file data is
much lower than expected

## Genre analyisis
To analyze genres, we tidy the data in a similar manner to the format tidying,
dropping all other columns, summarizing after a grouping with year, and
imputing for the rest of 2019. Since rock was the dominant genre, I also take
a look at non-rock genres, and then remove the "small genres" at the bottom
of the graph to make it easier to read. This can be treated like outlier
removal
```{r genres}
genres <- all_albums %>% filter(genre != 'None')
count_genre_per_year <- genres %>% select(genre, year) %>% 
  group_by(genre, year) %>% summarize(total = n())

count_genre_per_year <- count_genre_per_year %>% 
  mutate(total = ifelse(year == 2019, total * 4, total)) %>% filter(year > 1955)


ggplot(count_genre_per_year, aes(x = year, y=total, color=genre)) + geom_line()

count_genre_no_rock <- count_genre_per_year %>% filter(genre != 'Rock' & year > 1960)

ggplot(count_genre_no_rock, aes(x = year, y=total, color=genre)) + geom_line()

count_genre_just_big_ones <- count_genre_no_rock %>% 
  filter (genre != 'Non-Music' & genre != 'Children\'s' & genre != 'Classical' &
          genre != 'Stage & Screen' & genre != 'Reggae' & genre != 'Blues' & year != 2019)

ggplot(count_genre_just_big_ones, aes(x = year, y=total, color=genre)) + geom_line()
```

The genre graph was by far one of the more interesting that we had, as it shows
a vast number of cultural trends. There are too many to name here, but a few
examples to observe is the rise in hip-hop since the 80s (and ensuing fall of
rock in the 2010s that industry experts have been parroting for years), the
two Electronic booms, one in the 80s (sythwave), and a massive boom/bust in
the 2010s (the EDM Boom). Also observe the rise and fall and rise again of
Funk/Soul in the 70s, then returning in the 2010s.


## Challenges, limitations, and biases
This project had a number of challenges, limitations and biases that must be
considered that affected the data that we were working with. 

### Biases
First, it  is important to remember that we only looked at american pop music of the last 60
years, and not international music or music from other time periods.

Another bias that was previously mentioned has to do with Discogs and how their
tagging system works. Since discogs is a website dedicated to the tracking of
physical releases, users are heavily biased to tag an albums "primary release" 
with a physical release even when a file or digital release might be more
important to the work as a whole. This would help explain the low amount of
file data that we got

### Difficulties
Gathering the data in the method that we did proved to be extremely difficult
as the three APIs that we used were almost completely disparate save for the
fact that they tracked music. The first "API" was really just the Billboard
website and data from their webisite was very unclean, often with extra spaces,
symbols, or incorrect capitalization. This was fixed in the database stage,
with an SQL ```TRIM``` command.